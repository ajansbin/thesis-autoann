{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ZodData class...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8491b0cc7a334523ae6884fd0e78484a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading train sequences:   0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1de74db3774f24a0492c75ea4cd4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading val sequences:   0%|          | 0/294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prediction and ground-truths ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209945/209945 [00:00<00:00, 1845283.87it/s]\n",
      "100%|██████████| 1165/1165 [00:55<00:00, 20.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from smoother.data.zod_data import ZodTrackingResults\n",
    "from smoother.io.config_utils import load_config\n",
    "\n",
    "#config = '/home/s0001668/workspace/thesis-autoann/AutoAnnSmoother/configs/training_config.yaml'\n",
    "data_path = '/datasets/zod/zodv2'\n",
    "result_path = '/staging/agp/masterthesis/2023autoann/storage/tracking/SimpleTrack_zod_full_train/results/Vehicle/results_lidar.json'\n",
    "#result_path = '/staging/agp/masterthesis/2023autoann/storage/detection/CenterPoint/predictions/cp-zod-mini-results-train/pts_bbox/results_zod.json'\n",
    "#model_path = '/staging/agp/masterthesis/2023autoann/storage/smoothing/autoannsmoothing/pc-track/pc-track_zod_train_pctrack-w5-sw_model.pth'\n",
    "config = '/staging/agp/masterthesis/2023autoann/storage/smoothing/autoannsmoothing/pc-track/config.yaml'\n",
    "model_path = '/staging/agp/masterthesis/2023autoann/storage/smoothing/autoannsmoothing/pc-track/pc-track_zod_train_pctrack-w25-sw_model.pth'\n",
    "conf = load_config(config)\n",
    "tracking_results = ZodTrackingResults(result_path, conf, 'full', 'train', data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smoother.models.pc_track_net import PCTrackNet, TrackNet,PCNet \n",
    "import torch\n",
    "\n",
    "model = PCTrackNet(track_encoder='pointnet', \n",
    "                pc_encoder='pointnet', \n",
    "                decoder='pool', \n",
    "                pc_feat_dim=4, \n",
    "                track_feat_dim=9, \n",
    "                pc_out=256, \n",
    "                track_out=64, \n",
    "                dec_out=16)\n",
    "\n",
    "trained_model = model\n",
    "checkpoint = torch.load(model_path)\n",
    "trained_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1165/1165 [01:41<00:00, 11.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from smoother.data.common import transformations as t\n",
    "importlib.reload(t)\n",
    "from smoother.data.common import transformations as t\n",
    "\n",
    "from smoother.data.common import tracking_data as td\n",
    "importlib.reload(td)\n",
    "from smoother.data.common import tracking_data as td\n",
    "\n",
    "#center-offset means/stds\n",
    "means = [-1.0408938187128578, 15.676385645189473, -0.5681853294492015, 2.5877273679628194, 1.0347822035160879, 0.9536876676924065, -0.3136310277276827, -0.013401212689278484]\n",
    "stdev = [8.827627848858757, 19.47008377654335, 0.6877734215244837, 3.1708958713662074, 1.0381670406742867, 1.0549146278354404, 0.5951111005230132, 0.24225834247964198]\n",
    "normalize = t.Normalize(means, stdev)\n",
    "\n",
    "transformations = [t.ToTensor(), normalize, t.CenterOffset()]\n",
    "tracking_data = td.TrackingData(tracking_results, transformations)\n",
    "\n",
    "track_object = tracking_data.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_id': '000231', 'tracking_id': 'Vehicle_181_0', 'starting_frame_index': 0, 'track_length': 180, 'foi_index': 89, 'assoc_metric': 'giou', 'assoc_thresh': 0.0, 'has_gt': True, 'gt_dist': tensor(0.8270, dtype=torch.float64)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = td.WindowTrackingData(tracking_results, -12, 12, transformations, tracking_data)\n",
    "track_object = data_model.get(0)\n",
    "track_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCTrackNet(\n",
       "  (pc_encoder): PCEncoder(\n",
       "    (tnet4): TNet(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=16, bias=True)\n",
       "    )\n",
       "    (conv1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv4): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (track_encoder): TrackEncoder(\n",
       "    (tnet9): TNet(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (fc): Linear(in_features=64, out_features=81, bias=True)\n",
       "    )\n",
       "    (conv1): Conv1d(9, 32, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (temporal_decoder): PoolDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=320, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (heads): DecoderHeads(\n",
       "      (fc_center): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=3, bias=True)\n",
       "      )\n",
       "      (fc_size): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=3, bias=True)\n",
       "      )\n",
       "      (fc_rotation): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7461"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s0001668/workspace/thesis-autoann/AutoAnnSmoother/smoother/data/common/transformations.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.offset = torch.tensor(x[0:3], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 7461\n",
      "100 / 7461\n",
      "200 / 7461\n",
      "300 / 7461\n",
      "400 / 7461\n",
      "500 / 7461\n",
      "600 / 7461\n",
      "700 / 7461\n",
      "800 / 7461\n",
      "900 / 7461\n",
      "1000 / 7461\n",
      "1100 / 7461\n",
      "1200 / 7461\n",
      "1300 / 7461\n",
      "1400 / 7461\n",
      "1500 / 7461\n",
      "1600 / 7461\n",
      "1700 / 7461\n",
      "1800 / 7461\n",
      "1900 / 7461\n",
      "2000 / 7461\n",
      "2100 / 7461\n",
      "2200 / 7461\n",
      "2300 / 7461\n",
      "2400 / 7461\n",
      "2500 / 7461\n",
      "2600 / 7461\n",
      "2700 / 7461\n",
      "2800 / 7461\n",
      "2900 / 7461\n",
      "3000 / 7461\n",
      "3100 / 7461\n",
      "3200 / 7461\n",
      "3300 / 7461\n",
      "3400 / 7461\n",
      "3500 / 7461\n",
      "3600 / 7461\n",
      "3700 / 7461\n",
      "3800 / 7461\n",
      "3900 / 7461\n",
      "4000 / 7461\n",
      "4100 / 7461\n",
      "4200 / 7461\n",
      "4300 / 7461\n",
      "4400 / 7461\n",
      "4500 / 7461\n",
      "4600 / 7461\n",
      "4700 / 7461\n",
      "4800 / 7461\n",
      "4900 / 7461\n",
      "5000 / 7461\n",
      "5100 / 7461\n",
      "5200 / 7461\n",
      "5300 / 7461\n",
      "5400 / 7461\n",
      "5500 / 7461\n",
      "5600 / 7461\n",
      "5700 / 7461\n",
      "5800 / 7461\n",
      "5900 / 7461\n",
      "6000 / 7461\n",
      "6100 / 7461\n",
      "6200 / 7461\n",
      "6300 / 7461\n",
      "6400 / 7461\n",
      "6500 / 7461\n",
      "6600 / 7461\n",
      "6700 / 7461\n",
      "6800 / 7461\n",
      "6900 / 7461\n",
      "7000 / 7461\n",
      "7100 / 7461\n",
      "7200 / 7461\n",
      "7300 / 7461\n",
      "7400 / 7461\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from smoother.data.common.utils import convert_to_sine_cosine, convert_to_quaternion\n",
    "\n",
    "zod = ZodSequences(data_path, 'full')\n",
    "\n",
    "smoothing_results = {}\n",
    "smoothing_results['meta'] = {\n",
    "    'use_camera': False,\n",
    "    'use_lidar': True,\n",
    "     'use_radar': False,\n",
    "    'use_map': False,\n",
    "    'use_external': False\n",
    "    }\n",
    "smoothing_results['results'] = {}\n",
    "\n",
    "for i, (x1, x2, y) in enumerate(data_model):\n",
    "    if i % 100 == 0:\n",
    "        print(i, '/', len(data_model))\n",
    "\n",
    "    tracks, points, gt_anns = x1, x2, y #x1.to(self.device), x2.to(self.device), y.to(self.device)\n",
    "    track_object = data_model.get(i)\n",
    "\n",
    "    refined_track = trained_model.forward(tracks.unsqueeze(0), points.unsqueeze(0)).squeeze()\n",
    "    for transformation in reversed(transformations):\n",
    "        if type(transformation) == t.CenterOffset:\n",
    "            transformation.set_offset(track_object.center_offset)\n",
    "            transformation.set_start_and_end_index(0, -1)\n",
    "        if type(transformation) == t.Normalize:\n",
    "            transformation.set_start_and_end_index(0, -1)\n",
    "        refined_track = transformation.untransform(refined_track)\n",
    "\n",
    "    seq_id = track_object.sequence_id\n",
    "    \n",
    "    seq = zod[seq_id]\n",
    "    frame_token = os.path.basename(seq.info.get_key_lidar_frame().filepath)\n",
    "\n",
    "    refined_box = {\n",
    "        'sample_token': frame_token,\n",
    "        'translation': refined_track[:3].tolist(),\n",
    "        'size': refined_track[3:6].tolist(),\n",
    "        'rotation': convert_to_quaternion(refined_track[6:8].tolist()),\n",
    "        'velocity': [0,0],\n",
    "        'tracking_id': track_object.tracking_id,\n",
    "        'tracking_name': 'Vehicle',\n",
    "        'tracking_score': 0,\n",
    "    }\n",
    "\n",
    "    if seq_id not in smoothing_results['results']:\n",
    "        smoothing_results['results'][frame_token] = []\n",
    "    \n",
    "    smoothing_results['results'][frame_token].append(refined_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smoothing_results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "save_path = '/staging/agp/masterthesis/2023autoann/storage/smoothing/autoannsmoothing/results/full_train.json'\n",
    "mmcv.dump(smoothing_results, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foi [4.994181518529014, 22.647279932471744, -1.550068295257373]\n",
      "ref tensor([ 5.0743, 22.6973, -1.5385], grad_fn=<SliceBackward>)\n",
      "gt [5.124, 22.317, -1.44]\n"
     ]
    }
   ],
   "source": [
    "print('foi', foi_box.center)\n",
    "print('ref', refined_track[0:3])\n",
    "print('gt', track_object.gt_box['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for track_id, refined_tracks in track_ids.items():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.5593, 0.8350, 0.6963, 0.3102, 4.1486, 0.0000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-33.513233184814474, 38.131393432617216, -6.40397644042969]\n",
      "[-18.569011688232433, 38.04195404052737, -4.026726722717286]\n",
      "[-36.35507965087893, 38.54131317138675, -5.940477371215822]\n"
     ]
    }
   ],
   "source": [
    "for boxes in tracking_results.pred_boxes['000000_quebec_2022-02-14T13:23:32.251875Z.npy']:\n",
    "    for box in boxes:\n",
    "        print(box['translation'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd68b10c704c8b2adedec7d053ecf5e45a77a5521c9555d830c56f18df4089b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
